{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. KMeans Featurizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KMeansFeaturizer:\n",
    "    \"\"\"Transforms numeric data into k-means cluster memberships.\n",
    "    \n",
    "    This transformer runs k-means on the input data and converts each data point\n",
    "    into the id of the closest cluster. If a target variable is present, it is \n",
    "    scaled and included as input to k-means in order to derive clusters that\n",
    "    obey the classification boundary as well as group similar points together.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    k: integer, optional, default 100\n",
    "        The number of clusters to group data into.\n",
    "\n",
    "    target_scale: float, [0, infty], optional, default 5.0\n",
    "        The scaling factor for the target variable. Set this to zero to ignore\n",
    "        the target. For classification problems, larger `target_scale` values \n",
    "        will produce clusters that better respect the class boundary.\n",
    "\n",
    "    random_state : integer or numpy.RandomState, optional\n",
    "        This is passed to k-means as the generator used to initialize the \n",
    "        kmeans centers. If an integer is given, it fixes the seed. Defaults to \n",
    "        the global numpy random number generator.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    cluster_centers_ : array, [k, n_features]\n",
    "        Coordinates of cluster centers. n_features does count the target column.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, k=100, target_scale=5.0, random_state=None):\n",
    "        self.k = k\n",
    "        self.target_scale = target_scale\n",
    "        self.random_state = random_state\n",
    "        self.cluster_encoder = OneHotEncoder().fit(np.array(range(k)).reshape(-1,1))\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"Runs k-means on the input data and find centroids.\n",
    "\n",
    "        If no target is given (`y` is None) then run vanilla k-means on input `X`. \n",
    "\n",
    "        If target `y` is given, then include the target (weighted by `target_scale`) \n",
    "        as an extra dimension for k-means clustering. In this case, run k-means \n",
    "        twice, first with the target, then an extra iteration without.\n",
    "\n",
    "        After fitting, the attribute `cluster_centers_` are set to the k-means\n",
    "        centroids in the input space represented by `X`.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like or sparse matrix, shape=(n_data_points, n_features)\n",
    "\n",
    "        y : vector of length n_data_points, optional, default None\n",
    "            If provided, will be weighted with `target_scale` and included in \n",
    "            k-means clustering as hint.\n",
    "        \"\"\"\n",
    "        if y is None:\n",
    "            # No target variable, just do plain k-means\n",
    "            km_model = KMeans(n_clusters=self.k, \n",
    "                              n_init=20, \n",
    "                              random_state=self.random_state)\n",
    "            km_model.fit(X)\n",
    "\n",
    "            self.km_model = km_model\n",
    "            self.cluster_centers_ = km_model.cluster_centers_\n",
    "            return self\n",
    "\n",
    "        # There is target information. Apply appropriate scaling and include\n",
    "        # into input data to k-means            \n",
    "        data_with_target = np.hstack((X, y[:,np.newaxis]*self.target_scale))\n",
    "    \n",
    "        # Build a pre-training k-means model on data and target\n",
    "        km_model_pretrain = KMeans(n_clusters=self.k, \n",
    "                                   n_init=20, \n",
    "                                   random_state=self.random_state)\n",
    "        km_model_pretrain.fit(data_with_target)\n",
    "\n",
    "        # Run k-means a second time to get the clusters in the original space\n",
    "        # without target info. Initialize using centroids found in pre-training.\n",
    "        # Go through a single iteration of cluster assignment and centroid \n",
    "        # recomputation.\n",
    "        \n",
    "        km_model = KMeans(n_clusters=self.k, \n",
    "                          init=km_model_pretrain.cluster_centers_[:,:-1], # km_model_pretrain.cluster_centers_[:,:2], \n",
    "                          n_init=1, \n",
    "                          max_iter=1)\n",
    "        km_model.fit(X)\n",
    "        \n",
    "        self.km_model = km_model\n",
    "        self.cluster_centers_ = km_model.cluster_centers_\n",
    "        return self\n",
    "        \n",
    "    def transform(self, X, y=None):\n",
    "        \"\"\"Outputs the closest cluster id for each input data point.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like or sparse matrix, shape=(n_data_points, n_features)\n",
    "\n",
    "        y : vector of length n_data_points, optional, default None\n",
    "            Target vector is ignored even if provided.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        cluster_ids : array, shape[n_data_points,1]\n",
    "        \"\"\"\n",
    "        clusters = self.km_model.predict(X)\n",
    "        return self.cluster_encoder.transform(clusters.reshape(-1,1))\n",
    "    \n",
    "    def fit_transform(self, X, y=None):\n",
    "        \"\"\"Runs fit followed by transform.\n",
    "        \"\"\"\n",
    "        self.fit(X, y)\n",
    "        return self.transform(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Moons Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1\n",
    "# train, test data\n",
    "training_data, training_labels = make_moons(n_samples=2000, noise=0.2, random_state=seed)\n",
    "test_data, test_labels = make_moons(n_samples=2000, noise=0.3, random_state=seed+5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(numpy.ndarray, numpy.ndarray, (2000, 2), (2000,))"
      ]
     },
     "execution_count": 355,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(training_data), type(training_labels), training_data.shape, training_labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(random_state=1)"
      ]
     },
     "execution_count": 356,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model train\n",
    "clf = LogisticRegression(random_state=seed)\n",
    "clf.fit(training_data, training_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9356779999999999"
      ]
     },
     "execution_count": 357,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test score\n",
    "predictions = clf.predict_proba(test_data)[:,1]\n",
    "roc_auc_score(test_labels, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KMeans Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kmeans featurizer init\n",
    "kmf_hint = KMeansFeaturizer(k=5, target_scale=10, random_state=seed).fit(training_data, training_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kmeans feature for train\n",
    "training_cluster_features = kmf_hint.transform(training_data)\n",
    "training_cluster_features= training_cluster_features.todense()\n",
    "training_data = np.hstack((training_data, training_cluster_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(random_state=1)"
      ]
     },
     "execution_count": 360,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model train\n",
    "clf = LogisticRegression(random_state=seed)\n",
    "clf.fit(training_data, training_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kmeans feature for test \n",
    "test_cluster_features = kmf_hint.transform(test_data)\n",
    "test_cluster_features= test_cluster_features.todense()\n",
    "test_data = np.hstack((test_data, test_cluster_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.948393"
      ]
     },
     "execution_count": 362,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test score\n",
    "predictions = clf.predict_proba(test_data)[:,1]\n",
    "roc_auc_score(test_labels, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Titanic Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Titanic data\n",
    "df_titanic = (pd.read_csv('../data/titanic.csv')\n",
    "              .astype({'Survived':'object', 'Pclass':'object'})\n",
    "              .drop(columns=['PassengerId', 'Name', 'Ticket', 'Cabin'])\n",
    "              .replace({'Sex': {'male':0, 'female':1}})\n",
    "              .pipe(lambda df: pd.concat([df, pd.get_dummies(df['Pclass'], prefix='Pclass')], axis=1)).drop(columns='Pclass')\n",
    "              .pipe(lambda df: pd.concat([df, pd.get_dummies(df['Embarked'], prefix='Embarked')], axis=1)).drop(columns='Embarked')\n",
    "              .dropna()\n",
    "             )\n",
    "\n",
    "df_titanic_train, df_titanic_val = train_test_split(df_titanic, test_size=0.2)\n",
    "x_train = df_titanic_train.drop(columns=['Survived']).values\n",
    "y_train = np.array(df_titanic_train['Survived'].tolist())\n",
    "x_val = df_titanic_val.drop(columns=['Survived']).values\n",
    "y_val = np.array(df_titanic_val['Survived'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(numpy.ndarray, numpy.ndarray, (571, 11), (571,))"
      ]
     },
     "execution_count": 364,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(x_train), type(y_train), x_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(571, 11)\n"
     ]
    }
   ],
   "source": [
    "# model train\n",
    "clf = LogisticRegression(random_state=seed, max_iter=1000)\n",
    "clf.fit(x_train, y_train)\n",
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8077095472882115"
      ]
     },
     "execution_count": 367,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test score\n",
    "predictions = clf.predict_proba(x_val)[:,1]\n",
    "roc_auc_score(y_val, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KMeans feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmf = KMeansFeaturizer(k=4, target_scale=10, random_state=seed).fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kmeans feature for train\n",
    "training_cluster_features = kmf.transform(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_cluster_features= training_cluster_features.todense()\n",
    "x_train = np.hstack((x_train, training_cluster_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(max_iter=1000, random_state=1)"
      ]
     },
     "execution_count": 385,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model train\n",
    "clf = LogisticRegression(random_state=seed, max_iter=1000)\n",
    "clf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(143, 45)\n"
     ]
    }
   ],
   "source": [
    "# kmeans feature for test \n",
    "test_cluster_features = kmf.transform(x_val)\n",
    "test_cluster_features= test_cluster_features.todense()\n",
    "x_val = np.hstack((x_val, test_cluster_features))\n",
    "print(x_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baseline auc == 0.8099506947557149\n"
     ]
    }
   ],
   "source": [
    "# test score\n",
    "predictions = clf.predict_proba(x_val)[:,1]\n",
    "auc_base = roc_auc_score(y_val, predictions)\n",
    "print(f'baseline auc == {auc_base}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimize n_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_cluster == 2, auc == 0.8113\n",
      "n_cluster == 3, auc == 0.8088\n",
      "n_cluster == 4, auc == 0.8104\n",
      "n_cluster == 5, auc == 0.8097\n",
      "n_cluster == 6, auc == 0.8100\n",
      "n_cluster == 7, auc == 0.8073\n",
      "n_cluster == 8, auc == 0.8021\n",
      "n_cluster == 9, auc == 0.8086\n",
      "n_cluster == 10, auc == 0.8079\n",
      "n_cluster == 11, auc == 0.8048\n",
      "n_cluster == 12, auc == 0.8061\n",
      "n_cluster == 13, auc == 0.8079\n",
      "n_cluster == 14, auc == 0.8084\n",
      "n_cluster == 15, auc == 0.8048\n",
      "n_cluster == 16, auc == 0.8048\n",
      "n_cluster == 17, auc == 0.7972\n",
      "n_cluster == 18, auc == 0.8019\n",
      "n_cluster == 19, auc == 0.8008\n",
      "n_cluster == 20, auc == 0.7965\n",
      "n_cluster == 21, auc == 0.8037\n",
      "n_cluster == 22, auc == 0.8075\n",
      "n_cluster == 23, auc == 0.8055\n",
      "n_cluster == 24, auc == 0.8084\n",
      "n_cluster == 25, auc == 0.8082\n",
      "n_cluster == 26, auc == 0.8082\n",
      "n_cluster == 27, auc == 0.8030\n",
      "n_cluster == 28, auc == 0.8061\n",
      "n_cluster == 29, auc == 0.8064\n"
     ]
    }
   ],
   "source": [
    "def k_test(k, x_train, y_train, x_val, y_val):\n",
    "    # kmeans center init with x, y values\n",
    "    kmf = KMeansFeaturizer(k=k, target_scale=10, random_state=seed).fit(x_train, y_train)\n",
    "    \n",
    "    # kmeans feature for train\n",
    "    training_cluster_features = kmf.transform(x_train)\n",
    "\n",
    "    training_cluster_features= training_cluster_features.todense()\n",
    "    x_train = np.hstack((x_train, training_cluster_features))\n",
    "\n",
    "    # model train\n",
    "    clf = LogisticRegression(random_state=seed, max_iter=1000)\n",
    "    clf.fit(x_train, y_train)\n",
    "\n",
    "    # kmeans feature for test \n",
    "    test_cluster_features = kmf.transform(x_val)\n",
    "    test_cluster_features= test_cluster_features.todense()\n",
    "    x_val = np.hstack((x_val, test_cluster_features))\n",
    "\n",
    "    # test score\n",
    "    predictions = clf.predict_proba(x_val)[:,1]\n",
    "    auc = roc_auc_score(y_val, predictions)\n",
    "    \n",
    "    return auc\n",
    "    \n",
    "n_clusters = list(range(2, 30))\n",
    "auc_history = []\n",
    "for n_cluster in n_clusters:\n",
    "    auc = k_test(n_cluster, x_train, y_train, x_val, y_val)\n",
    "    print(f'n_cluster == {n_cluster}, auc == {auc:.4f}')\n",
    "    auc_history.append(auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 0.811295383236217)"
      ]
     },
     "execution_count": 389,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_clusters[np.argmax(auc_history)], np.max(auc_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ baseline auc : 0.8077\n",
    "+ kmeans featurizer auc : 0.8112(n_clusters == 2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
